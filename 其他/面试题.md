# 面试题

## 深度学习理论

## A

### 精确率（Precision）和召回率（Recall）以及F1分数（F1 score）计算方式，且表达了什么内容

 精确率/查准率：P = TP/(TP+FP)；识别出的所有目标中识别正确的

召回率/查全率：R = TP/(TP+FN)；正确识别出所有目标中的多少

F1值：F1 = 2*(P*R)/(P+R)，精确率和召回率的调和均值。综合表达PR值

### AP计算方式

PR曲线下面积，曲线绘制根据置信度排序从大到小排序。

### 激活函数的用处

提供了非线性单元，使得整个网络变为非线性，从而能够解决各种非线性问题。

### BN有什么作用

加速训练；

防止过拟合；

缓解梯度消失爆炸；

降低参数初始化要求；

### pooling作用

减小网络参数量；

减小计算量；

## B

### 数据不均衡该怎么处理

数据过采样；

数据欠采样；

数据增广；

损失类别加权；

### Sigmoid和ReLU各自优缺点

sigmoid

优点：数据归一化；

缺点：可能引发梯度消失；

ReLU

优点：解决梯度消失和爆炸；计算速度快；

缺点：输出在负半轴的神经元会死掉；

### 分割项目目标边界不清晰怎么处理

边界区域不参与训练；

### ResNet解决了什么问题

ResNet提出是为了解决或缓解深度神经网络训练中的梯度消失问题。通过增加shortcut，使得梯度多了一个传递的途径，让更深的网络成为可能。

### 图像处理使用卷积神经网络还不是用全连接网络

首先，CNN相对于FC的参数量减少非常多，对于图像这种输入维度相对较大的任务，全部使用FC不现实，另外参数量过多而数据规模跟不上非常容易过拟合，网络本身也难以训练。图像本身附近像素的关联信息很多，CNN正好能够提取一个区域数据的特征，并且能够通过不断加深扩展感受野，使得其适用于图像任务。

### 梯度消失和爆炸的解决方法

梯度消失/爆炸解决方法：
1. 使用深度更浅的模型（针对梯度爆炸问题）；
2. 使用类似ReLU（ReLU的贡献①解决了梯度消失和爆炸②计算速度快），为解决负半轴死掉的神经元可使用类似LeakReLU的激活函数；
3. 在循环神经网络结构中使用LSTM可以减少梯度爆炸问题，主要是门结构的作用；
4. 梯度截断，当梯度超过阈值时，取阈值为梯度（解决梯度爆炸问题）；
5. 使用权重正则化/批量归一化（针对梯度爆炸）；
6. 残差结构；

### 如何防止过拟合

数据增广；

正则化；

dropout；

early stop；

### focal loss原理

通过减少易分类样本的权重，使得模型在训练时更专注于难分类的样本。

## C

### 讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？

作用：对输入的特征图进行压缩，

一方面使特征图变小，简化网络计算复杂度；

一方面进行特征压缩，提取主要特征。

通常来讲，max-pooling的效果更好，虽然max-pooling和average-pooling都对数据做了下采样，但是max-pooling感觉更像是做了特征选择，选出了分类辨识度更好的特征，提供了非线性。 pooling的主要作用一方面是去掉冗余信息，一方面要保留feature map的特征信息，在分类问题中，我们需要知道的是这张图像有什么object，而不大关心这个object位置在哪，在这种情况下显然max pooling比average pooling更合适。在网络比较深的地方，特征已经稀疏了，从一块区域里选出最大的，比起这片区域的平均值来，更能把稀疏的特征传递下去。

average-pooling更强调对整体特征信息进行一层下采样，在减少参数维度的贡献上更大一点，更多的体现在信息的完整传递这个维度上，在一个很大很有代表性的模型中，比如说DenseNet中的模块之间的连接大多采用average-pooling，在减少维度的同时，更有利信息传递到下一个模块进行特征提取。

average-pooling在全局平均池化操作中应用也比较广，在ResNet和Inception结构中最后一层都使用了平均池化。有的时候在模型接近分类器的末端使用全局平均池化还可以代替Flatten操作，使输入数据变成一位向量。

### Pooling层怎么反向传播

mean pooling

![img](https://upload-images.jianshu.io/upload_images/17624987-9de749ff8ff95a88.png?imageMogr2/auto-orient/strip|imageView2/2/format/webp)

max pooling

![img](https://upload-images.jianshu.io/upload_images/17624987-e3b309cbe999aae4.png?imageMogr2/auto-orient/strip|imageView2/2/w/827/format/webp)

### 为什么LSTM比RNN好

PS：RNN由于是多时刻权重共享，所以反向传播可能到早时间的梯度会爆炸或者消失，但是较晚时间的梯度是存在的（反向传播使用BPTT），最多只是早期信息不容易被学到。

因为多个门的存在，使得长时序的信息可以被学习到。

### SGD、SGDM、RMSprop和Adam优化器的关系

SGDM=SGD+动量

RMSprop=SGDM+权重衰减

Adam=RMSprop使用二阶求导

## python题

### 描述list、tuple的相同点和不同点

有序；

可嵌套；

元素可接受各种类别；

### is和==的区别

is指针判断；

==值判断

### With打开文件做了什么操作

```python
try:
    pass
except:
    pass
finally:
    pass
```

### 面向对象的特点优势和劣势

定义	面向对象顾名思义就是把现实中的事务都抽象成为程序设计中的“对象”，其基本思想是一切皆对象，是一种“自下而上”的设计语言，先设计组件，再完成拼装。
特点	封装、继承、多态
优势	适用于大型复杂系统，方便复用
劣势	比较抽象、性能比面向过程低

### 生成器，迭代器的区别

生成器能做到迭代器能做的所有事，而且因为自动创建iter()和next()方法，生成器显得特别简洁，而且生成器也是高效的，使用生成器表达式取代列表解析可以同时节省内存。

除了创建和保存程序状态的自动方法，当发生器终结时，还会自动抛出StopIteration异常。

## 开放题

### 深度学习中算法创新和增加数据你更偏向哪个？具体是怎么思考的呢？

### 各种常用网络（分类、检测、分割）

分类网络：ResNet，SENet，EfficientNet等；

检测网络：Faster RCNN，YOLO，SSD等；

分割网络：Mask RCNN、UNet、Deeplab等；

