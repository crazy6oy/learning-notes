# Base

## L0 L1 L2

### 概念

L0正则化的值是模型参数中非零参数的个数。

L1正则化表示各个参数绝对值之和。

L2正则化标识各个参数的平方的和的开方值。

### 前情引入

参数稀疏化好处：简化模型，防止过拟合。因为参数越多说明模型拟合训练数据越强，泛化性能越差。

### L0

具体操作可以参考ReLU函数，用非零特征来表示特征。但是这样会带来一些求导问题（但我不明白和L1有什么区别，我目前能看到的就是L1在负半轴是持续导数不为0的）。

### L1

也被称为lasso回归。就是每个参数取绝对值。

### L2

也被称为岭回归。参数先平方在开平方根。他和L1不一样的是他不会让每个元素为0，而是接近0。

### 比较L1和L2

在二维坐标上随机绘制目标函数等高线，又由于目标函数一般会选择凸函数，所以目标函数和L1、L2函数在坐标系中交点，L1相比L2有更大概率在坐标轴上与目标函数相交（的到最优解），所以会产生稀疏参数。

L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。

## 梯度下降法和牛顿法的优缺点

### 优点

梯度下降法：可用于数据量较大的情况。

牛顿法：收敛速度更快。

### 缺点

梯度下降法：不是每一步都向着最优解方向。

牛顿法：每次迭代时间长，需要算一阶和二阶导数。

## 解决训练中样本类别不平衡

过采样：复制样本数较少的类别样本；

欠采样：删除样本数较大的类别样本；

类别加权：根据类别样本数设置学习权重，样本数较少的类别权重较大。

## NN（神经网络）正则方法

L1、L2、dropout

## Batch Normalization

在批次的维度上做均值和方差计算，具体内容还会对这个结果进行缩放和平移

![image-20220329201017493](C:\Users\wangyx\AppData\Roaming\Typora\typora-user-images\image-20220329201017493.png)

1. 限制参数对隐层数据分布的影响，使其始终保持均值为0，方差为1的分布；

2. 削弱了前层参数和后层参数之间的联系，使得当前层稍稍独立于其他层，加快收敛速度；

3. 有正则化效果。

## Momentum优化

原理：在梯度下降算法中在一定程度上保留了之前梯度更新的方向，同时利用当前mini_batch的梯度方向微调最终的更新方向。

作用：在一定程度上增加梯度更新方向的稳定性，在一等程度上可冲过局部最优解，从而使得收敛速度更快。

## 梯度消失梯度爆炸

顾名思义，就是在反向传播计算的时候梯度无限接近0或非常大。

梯度消失引发的原因：

1. 神经网络结构；
2. 不合适的激活函数，如sigmoid。

梯度爆炸引发的原因：

1. 神经网络结构；
2. 网络参数初始化值太大；
3. 不合适的激活函数。

梯度消失/爆炸解决方法：

1. 使用深度更浅的模型（针对梯度爆炸问题）；
2. 使用类似ReLU（ReLU的贡献①解决了梯度消失和爆炸②计算速度快），为解决负半轴死掉的神经元可使用类似LeakReLU的激活函数；
3. 在循环神经网络结构中使用LSTM可以减少梯度爆炸问题，主要是门结构的作用；
4. 梯度截断，当梯度超过阈值时，取阈值为梯度（解决梯度爆炸问题）；
5. 使用权重正则化/批量归一化（针对梯度爆炸）；
6. 残差结构；

## 生成模型和判别模型



后续

https://blog.csdn.net/attitude_yu/article/details/80963323